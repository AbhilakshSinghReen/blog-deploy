3:I[9275,[],""]
5:I[1343,[],""]
6:I[6871,["98","static/chunks/98-7f6363f9ac68c942.js","748","static/chunks/748-1f14ee6eb3d7029a.js","271","static/chunks/271-37a22bace913f735.js","185","static/chunks/app/layout-08611f287dda1230.js"],"default"]
7:I[5877,["98","static/chunks/98-7f6363f9ac68c942.js","748","static/chunks/748-1f14ee6eb3d7029a.js","271","static/chunks/271-37a22bace913f735.js","185","static/chunks/app/layout-08611f287dda1230.js"],"default"]
8:I[902,["98","static/chunks/98-7f6363f9ac68c942.js","748","static/chunks/748-1f14ee6eb3d7029a.js","271","static/chunks/271-37a22bace913f735.js","185","static/chunks/app/layout-08611f287dda1230.js"],""]
9:I[5218,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
a:I[2324,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],"default"]
b:I[2591,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
c:I[231,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
d:I[3180,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","160","static/chunks/app/not-found-ae5c290377385b28.js"],""]
e:I[3245,["98","static/chunks/98-7f6363f9ac68c942.js","748","static/chunks/748-1f14ee6eb3d7029a.js","271","static/chunks/271-37a22bace913f735.js","185","static/chunks/app/layout-08611f287dda1230.js"],"default"]
4:["categorySlug","ml-ops","d"]
0:["VvaX91qL0y6klITKNgfgL",[[["",{"children":[["categorySlug","ml-ops","d"],{"children":["__PAGE__?{\"categorySlug\":\"ml-ops\"}",{}]}]},"$undefined","$undefined",true],["",{"children":[["categorySlug","ml-ops","d"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"en","suppressHydrationWarning":true,"children":["$","body",null,{"className":"__className_d65c78","children":["$","$L6",null,{"children":[["$","$L7",null,{}],["$","$L8",null,{"maxWidth":"lg","children":[["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$L9",null,{"children":[["$","$La",null,{"render":false,"targetSiteNavigationState":[],"targetPageNavigationState":[]}],["$","$Lb",null,{"variant":"h4","children":["$","strong",null,{"children":"Not Found"}]}],["$","$Lb",null,{"variant":"h6","mb":3,"children":"Oops! It looks like the page you're looking for does not exist."}],["$","$Lc",null,{"href":"/","children":["$","$Ld",null,{"variant":"contained","sx":{"marginRight":5,"marginBottom":1},"children":"Take me Home"}]}]]}],"notFoundStyles":[],"styles":null}],["$","$Le",null,{}]]}]]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/1d4d01086947be98.css","precedence":"next","crossOrigin":"$undefined"}]],"$Lf"]]]]
f:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","link","2",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
1:null
10:I[3507,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
11:I[6652,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
12:I[6981,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
13:I[2818,["98","static/chunks/98-7f6363f9ac68c942.js","134","static/chunks/134-9729e0afcc6dfdfe.js","830","static/chunks/830-8a0df588e7c84864.js","335","static/chunks/app/%5BcategorySlug%5D/page-5e6eac2c1265ac2f.js"],""]
2:["$","$L9",null,{"mb":5,"pb":2,"sx":{"width":"100%","display":"flex","flexDirection":"column","justifyContent":"flex-start","alignItems":"center"},"children":[["$","$La",null,{"render":true,"targetSiteNavigationState":[{"absoluteSlug":"/ml-ops","title":"ML Ops"}],"targetPageNavigationState":[{"absoluteSlug":"/ml-ops/tensorflow-training-plus-conversion-to-onnx-and-tfjs","title":"Training a CNN in TensorFlow and Exporting it to ONNX and TFJS"},{"absoluteSlug":"/ml-ops/tensorflow-js-inference-in-react","title":"TensorFlow.js Inference in React on an image drawn with Konva"},{"absoluteSlug":"/ml-ops/total-segmentator-api-with-huey","title":"Running ML inference through a Huey Task Queue and FastAPI"},{"absoluteSlug":"/ml-ops/gpu-vs-cpu-performance-for-ml-inference-workflows","title":"ML Inference Performance on GPU and CPU across different batch sizes."},{"absoluteSlug":"/ml-ops/node-express-onnx-backend","title":"Express.js API for Inference using an ONNX Model"},{"absoluteSlug":"/ml-ops/fastapi-torch-backend","title":"Serving a PyTorch Model using FastAPI"},{"absoluteSlug":"/ml-ops/fastapi-onnx-backend","title":"Serving an ONNX Model using FastAPI"},{"absoluteSlug":"/ml-ops/onnxruntime-web-inference-in-react","title":"ONNXRuntime Inference in a React App"},{"absoluteSlug":"/ml-ops/fastapi-tensorflow-backend","title":"Serving a TensorFlow Model using FastAPI"},{"absoluteSlug":"/ml-ops/ml-inference-on-backend-vs-frontend","title":"ML Inference on Backend vs Frontend"},{"absoluteSlug":"/ml-ops/torch-training-and-conversion-to-onnx","title":"Training a CNN in PyTorch and Exporting it to ONNX"}]}],["$","$L10",null,{"container":true,"spacing":0,"mb":0,"children":[["$","$L10",null,{"item":true,"xs":12,"sm":6,"md":4,"children":["$","$L9",null,{"sx":{"width":"100%","height":"100%","display":"flex","flexDirection":"column","justifyContent":"center","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"h3","component":"h1","children":"ML Ops"}]}]}],["$","$L10",null,{"item":true,"xs":12,"sm":6,"md":8,"children":["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"column","justifyContent":"center","alignItems":"center"},"children":["$","img",null,{"src":"/media/images/ml-ops-category-thumbnail.jpg","style":{"width":"100%","maxWidth":"75vw","height":"auto","maxHeight":"35vh","objectFit":"cover"}}]}]}]]}],["$","$L10",null,{"container":true,"spacing":2,"mt":4,"mb":4,"children":[["$","$L10","/ml-ops/tensorflow-training-plus-conversion-to-onnx-and-tfjs",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/tensorflow-training-plus-conversion-to-onnx-and-tfjs","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/tensorflow-training-plus-conversion-to-onnx-and-tfjs-thumbnail.jpg","alt":"Training a CNN in TensorFlow and Exporting it to ONNX and TFJS","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Training a CNN in TensorFlow and Exporting it to ONNX and TFJS"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Learn how to train a simple CNN in TensorFlow and how to convert it to ONNX or TensorFlow.js for deployment."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"05m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/tensorflow-js-inference-in-react",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/tensorflow-js-inference-in-react","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/react-digit-recog-app-final.jpg","alt":"TensorFlow.js Inference in React on an image drawn with Konva","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","TensorFlow.js Inference in React on an image drawn with Konva"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Inferencing on an ONNX model in a React App using ONNXRuntime Web."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"08m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/total-segmentator-api-with-huey",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/total-segmentator-api-with-huey","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/total-segmentator-api-with-huey-system-architecture-diagram.jpg","alt":"Running ML inference through a Huey Task Queue and FastAPI","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Running ML inference through a Huey Task Queue and FastAPI"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Learn how to train a simple CNN in PyTorch and how to convert it to ONNX for deployment."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"07m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/gpu-vs-cpu-performance-for-ml-inference-workflows",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/gpu-vs-cpu-performance-for-ml-inference-workflows","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/gpu-vs-cpu-performance-for-ml-inference-workflows-thumbnail.jpg","alt":"ML Inference Performance on GPU and CPU across different batch sizes.","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","ML Inference Performance on GPU and CPU across different batch sizes."]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"A comparison of ML inference speed and memory consumption across various batch sizes on both GPU and CPU."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"08m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/node-express-onnx-backend",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/node-express-onnx-backend","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/node-express-onnx-backend-thumbnail.jpg","alt":"Express.js API for Inference using an ONNX Model","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Express.js API for Inference using an ONNX Model"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Deploying an ONNX Model using Express.js."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"08m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/fastapi-torch-backend",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/fastapi-torch-backend","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/fastapi-torch-backend-thumbnail.jpg","alt":"Serving a PyTorch Model using FastAPI","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Serving a PyTorch Model using FastAPI"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Learn how to serve a PyTorch model with FastAPI."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"06m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/fastapi-onnx-backend",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/fastapi-onnx-backend","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/fastapi-onnx-backend-thumbnail.jpg","alt":"Serving an ONNX Model using FastAPI","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Serving an ONNX Model using FastAPI"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Learn how to serve an ONNX model with FastAPI."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"06m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/onnxruntime-web-inference-in-react",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/onnxruntime-web-inference-in-react","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/react-digit-recog-app-final.jpg","alt":"ONNXRuntime Inference in a React App","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","ONNXRuntime Inference in a React App"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Inferencing on an ONNX model in a React App using ONNXRuntime Web."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"08m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/fastapi-tensorflow-backend",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/fastapi-tensorflow-backend","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/fastapi-tensorflow-backend-thumbnail.jpg","alt":"Serving a TensorFlow Model using FastAPI","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Serving a TensorFlow Model using FastAPI"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Learn how to serve a TensorFlow model with FastAPI."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"05m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/ml-inference-on-backend-vs-frontend",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/ml-inference-on-backend-vs-frontend","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/ml-inference-on-backend-vs-frontend-thumbnail.jpg","alt":"ML Inference on Backend vs Frontend","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","ML Inference on Backend vs Frontend"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"A comparison of ML inference speed and memory consumption across various batch sizes on both GPU and CPU."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"05m Read"}]}],false]}]]}]}]}],["$","$L10","/ml-ops/torch-training-and-conversion-to-onnx",{"item":true,"xs":12,"sm":6,"md":4,"lg":3,"children":["$","$Lc",null,{"href":"/ml-ops/torch-training-and-conversion-to-onnx","passHref":true,"style":{"textDecoration":"none"},"children":["$","$L11",null,{"sx":{"width":"100%","height":"100%","boxShadow":"1px 1px 1px 1px \"#eeeeee\"","&:hover":{"boxShadow":"3px 3px 3px 3px \"#eeeeee\"","cursor":"pointer"}},"children":[["$","$L12",null,{"component":"img","height":"150","image":"/media/images/torch-training-and-conversion-to-onnx-thumbnail.jpg","alt":"Training a CNN in PyTorch and Exporting it to ONNX","sx":{"objectFit":"cover"}}],["$","$L13",null,{"children":[["$","$Lb",null,{"gutterBottom":true,"variant":"h6","component":"h2","children":[""," ","Training a CNN in PyTorch and Exporting it to ONNX"]}],["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","sx":{"overflow":"hidden","textOverflow":"ellipsis","display":"-webkit-box","WebkitLineClamp":"3","WebkitBoxOrient":"vertical"},"children":"Learn how to train a simple CNN in PyTorch and how to convert it to ONNX for deployment."}],["$","$L9",null,{"sx":{"width":"100%","display":"flex","flexDirection":"row","justifyContent":"flex-end","alignItems":"center"},"children":["$","$Lb",null,{"gutterBottom":true,"variant":"p","component":"p","children":"07m Read"}]}],false]}]]}]}]}]]}]]}]
